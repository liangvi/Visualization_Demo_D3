{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack, coo_matrix\n",
    "\n",
    "import string\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import nltk\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = pd.read_json('yelp_dataset/business.json', lines=True)\n",
    "#checkin = pd.read_json('yelp_dataset/checkin.json', lines=True)\n",
    "#photo = pd.read_json('yelp_dataset/photo.json', lines=True)\n",
    "#review =  pd.read_json('yelp_dataset/review.json', lines=True)\n",
    "#tip = pd.read_json('yelp_dataset/tip.json', lines=True)\n",
    "##user = pd.read_json('yelp_dataset/user.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818 E Camino Acequia Drive</td>\n",
       "      <td>{'GoodForKids': 'False'}</td>\n",
       "      <td>1SWheh84yJXfytovILXOAQ</td>\n",
       "      <td>Golf, Active Life</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>33.522143</td>\n",
       "      <td>-112.018481</td>\n",
       "      <td>Arizona Biltmore Golf Club</td>\n",
       "      <td>85016</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30 Eglinton Avenue W</td>\n",
       "      <td>{'RestaurantsReservations': 'True', 'GoodForMe...</td>\n",
       "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
       "      <td>Specialty Food, Restaurants, Dim Sum, Imported...</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>{'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...</td>\n",
       "      <td>1</td>\n",
       "      <td>43.605499</td>\n",
       "      <td>-79.652289</td>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "      <td>L5R 3E7</td>\n",
       "      <td>128</td>\n",
       "      <td>2.5</td>\n",
       "      <td>ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10110 Johnston Rd, Ste 15</td>\n",
       "      <td>{'GoodForKids': 'True', 'NoiseLevel': 'u'avera...</td>\n",
       "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
       "      <td>Sushi Bars, Restaurants, Japanese</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>{'Monday': '17:30-21:30', 'Wednesday': '17:30-...</td>\n",
       "      <td>1</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "      <td>28210</td>\n",
       "      <td>170</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15655 W Roosevelt St, Ste 237</td>\n",
       "      <td>None</td>\n",
       "      <td>xvX2CttrVhyG2z1dFg_0xw</td>\n",
       "      <td>Insurance, Financial Services</td>\n",
       "      <td>Goodyear</td>\n",
       "      <td>{'Monday': '8:0-17:0', 'Tuesday': '8:0-17:0', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.455613</td>\n",
       "      <td>-112.395596</td>\n",
       "      <td>Farmers Insurance - Paul Lorenz</td>\n",
       "      <td>85338</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4209 Stuart Andrew Blvd, Ste F</td>\n",
       "      <td>{'BusinessAcceptsBitcoin': 'False', 'ByAppoint...</td>\n",
       "      <td>HhyxOkGAM07SRYtlQ4wMFQ</td>\n",
       "      <td>Plumbing, Shopping, Local Services, Home Servi...</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>{'Monday': '7:0-23:0', 'Tuesday': '7:0-23:0', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>35.190012</td>\n",
       "      <td>-80.887223</td>\n",
       "      <td>Queen City Plumbing</td>\n",
       "      <td>28217</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          address  \\\n",
       "0     2818 E Camino Acequia Drive   \n",
       "1            30 Eglinton Avenue W   \n",
       "2       10110 Johnston Rd, Ste 15   \n",
       "3   15655 W Roosevelt St, Ste 237   \n",
       "4  4209 Stuart Andrew Blvd, Ste F   \n",
       "\n",
       "                                          attributes             business_id  \\\n",
       "0                           {'GoodForKids': 'False'}  1SWheh84yJXfytovILXOAQ   \n",
       "1  {'RestaurantsReservations': 'True', 'GoodForMe...  QXAEGFB4oINsVuTFxEYKFQ   \n",
       "2  {'GoodForKids': 'True', 'NoiseLevel': 'u'avera...  gnKjwL_1w79qoiV3IC_xQQ   \n",
       "3                                               None  xvX2CttrVhyG2z1dFg_0xw   \n",
       "4  {'BusinessAcceptsBitcoin': 'False', 'ByAppoint...  HhyxOkGAM07SRYtlQ4wMFQ   \n",
       "\n",
       "                                          categories         city  \\\n",
       "0                                  Golf, Active Life      Phoenix   \n",
       "1  Specialty Food, Restaurants, Dim Sum, Imported...  Mississauga   \n",
       "2                  Sushi Bars, Restaurants, Japanese    Charlotte   \n",
       "3                      Insurance, Financial Services     Goodyear   \n",
       "4  Plumbing, Shopping, Local Services, Home Servi...    Charlotte   \n",
       "\n",
       "                                               hours  is_open   latitude  \\\n",
       "0                                               None        0  33.522143   \n",
       "1  {'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...        1  43.605499   \n",
       "2  {'Monday': '17:30-21:30', 'Wednesday': '17:30-...        1  35.092564   \n",
       "3  {'Monday': '8:0-17:0', 'Tuesday': '8:0-17:0', ...        1  33.455613   \n",
       "4  {'Monday': '7:0-23:0', 'Tuesday': '7:0-23:0', ...        1  35.190012   \n",
       "\n",
       "    longitude                             name postal_code  review_count  \\\n",
       "0 -112.018481       Arizona Biltmore Golf Club       85016             5   \n",
       "1  -79.652289       Emerald Chinese Restaurant     L5R 3E7           128   \n",
       "2  -80.859132      Musashi Japanese Restaurant       28210           170   \n",
       "3 -112.395596  Farmers Insurance - Paul Lorenz       85338             3   \n",
       "4  -80.887223              Queen City Plumbing       28217             4   \n",
       "\n",
       "   stars state  \n",
       "0    3.0    AZ  \n",
       "1    2.5    ON  \n",
       "2    4.0    NC  \n",
       "3    5.0    AZ  \n",
       "4    4.0    NC  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    \"\"\"\n",
    "    Modified from\n",
    "    http://adataanalyst.com/scikit-learn/countvectorizer-sklearn-example/\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation, and digits \n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    stemmer = EnglishStemmer()\n",
    "   \n",
    "    # Check characters to see if they are in punctuation\n",
    "    clean = [char for char in text if (char not in string.punctuation) \n",
    "            and (not char.isdigit())] \n",
    " \n",
    "    clean = ''.join(clean)\n",
    "    tokens = clean.split()\n",
    "    tokens = [stemmer.stem(c) for c in tokens]\n",
    "    # Join the characters again to form the string.\n",
    "\n",
    "    tokens = ' '.join(tokens)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only look at restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lv_restaurants\n",
    "restaurants = business['categories'].str.contains('Restaurants', regex=False)\n",
    "restaurants = restaurants.fillna(False)\n",
    "restaurants = business[restaurants]\n",
    "print(restaurants.shape)\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categories_df = pd.Series(restaurants['categories']).str.get_dummies(sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categories_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "totals = categories_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.barh(totals[11:1:-1].keys(),totals[11:1:-1], 0.5,  align='center')\n",
    "plt.title('Top Ten Restaurants Categories')\n",
    "plt.ylabel('Categories')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nightlife = restaurants['categories'].str.contains('Nightlife', regex=False)\n",
    "nightlife = nightlife.fillna(False)\n",
    "nightlife = restaurants[nightlife]\n",
    "\n",
    "pizza = restaurants['categories'].str.contains('Pizza', regex=False)\n",
    "pizza = pizza.fillna(False)\n",
    "pizza = restaurants[pizza]\n",
    "\n",
    "burgers = restaurants['categories'].str.contains('Burgers', regex=False)\n",
    "burgers = burgers.fillna(False)\n",
    "burgers = restaurants[burgers]\n",
    "burgers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(nightlife['stars'], bins = 10)\n",
    "plt.title('Distribution of Stars for Nightlife')\n",
    "plt.ylabel('Restaurants with Ratings')\n",
    "plt.xlabel('Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(pizza['stars'], bins = 10)\n",
    "plt.title('Distribution of Stars for Pizza')\n",
    "plt.ylabel('Restaurants with Ratings')\n",
    "plt.xlabel('Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(burgers['stars'], bins = 10)\n",
    "plt.title('Distribution of Stars for Burgers')\n",
    "plt.ylabel('Restaurants with Ratings')\n",
    "plt.xlabel('Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(categories_df, \\\n",
    "                                                    restaurants['stars'], test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#X_train = pd.Series(X_train).str.get_dummies(sep=', ')\n",
    "#X_test = pd.Series(X_test).str.get_dummies(sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized value to get categories\n",
    "#vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "#categories_count = vectorizer.fit_transform(restaurants['categories']).astype(np.int8)\n",
    "#categories_list = vectorizer.get_feature_names()\n",
    "#categories_df = pd.DataFrame(categories_count.toarray())\n",
    "#categories_df.columns = categories_list\n",
    "#categories_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#check for columns missing from test set\n",
    "#missing_train = list(set(list(X_train)) - set(list(X_test)))\n",
    "#for col in missing_train:\n",
    "#    X_test[col] = 0\n",
    "    \n",
    "#missing_test = list(set(list(X_test)) - set(list(X_train)))\n",
    "#for col in missing_test:\n",
    "#    X_train[col] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "y_train_cut = pd.cut(np.array(y_train), 2, labels=[1, 2])\n",
    "y_test_cut = pd.cut(np.array(y_test), 2, labels=[1, 2])\n",
    "np.array(y_train_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, np.array(y_train_cut)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array(y_test_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neigh.score(X_test, np.array(y_test_cut))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy_score(y_test_cut, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reg = linear_model.Ridge(alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "y_pred = pd.Series(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r2_score(y_test, y_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regL = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regL.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = regL.predict(X_test)\n",
    "y_pred = pd.Series(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r2_score(y_test, y_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Whole dataset, but running into memory issues\n",
    "#start = datetime.datetime.now()\n",
    "\n",
    "#iter_review =  pd.read_json('yelp_dataset/review.json', lines=True, chunksize=500)\n",
    "#reviews_df = pd.concat([df[df['business_id'].isin(set(df['business_id']).intersection(set(restaurants['business_id'])))] for df in iter_review])\n",
    "\n",
    "#end = datetime.datetime.now()\n",
    "#elapse = end-start\n",
    "#print(elapse.seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviews_df_file = open(\"reviews_df_file\",'wb')\n",
    "pickle.dump(reviews_df,reviews_df_file)\n",
    "reviews_df_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviews_df_file = open(\"reviews_df_file\",'rb')\n",
    "reviews_df_2 = pickle.load(reviews_df_file)\n",
    "reviews_df_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviews_df = pd.DataFrame()\n",
    "start = datetime.datetime.now()\n",
    "for df in pd.read_json('yelp_dataset/review.json', lines=True,  chunksize=1000):\n",
    "    reviews_df = df[df['business_id'].isin(set(df['business_id']).intersection(set(restaurants['business_id'])))]\n",
    "end = datetime.datetime.now()\n",
    "elapse = end-start\n",
    "print(elapse.seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviews_df.stars.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviews_df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keep the size to 1000 features for size\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "reviews_df.text = reviews_df.text.apply(text_process)\n",
    "vectorizer = TfidfVectorizer(analyzer='word',min_df=10, stop_words = stop_words.ENGLISH_STOP_WORDS, \n",
    "                             ngram_range=(1, 2))\n",
    "text_tfidf = vectorizer.fit_transform(reviews_df.text)\n",
    "text_features = vectorizer.get_feature_names()\n",
    "text_sparse_df = pd.SparseDataFrame(text_tfidf)\n",
    "text_sparse_df.columns = text_features\n",
    "text_sparse_df.fillna(0.0, inplace=True)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "elapse = end-start\n",
    "\n",
    "print(elapse.seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviews_df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word',min_df=5, stop_words = stop_words.ENGLISH_STOP_WORDS, \n",
    "                             ngram_range=(1, 1))\n",
    "text_count = count_vectorizer.fit_transform(reviews_df.text)\n",
    "text_count_features = count_vectorizer.get_feature_names()\n",
    "text_count_df = pd.SparseDataFrame(text_count)\n",
    "text_count_df.columns = text_count_features\n",
    "text_count_df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_count_totals = text_count_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "plt.barh(text_count_totals[11::-1].keys(),text_count_totals[11::-1], 0.5,  align='center')\n",
    "plt.title('Top Ten Words in Reviews')\n",
    "plt.ylabel('Words')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "countn_vectorizer = CountVectorizer(analyzer='word',min_df=5, stop_words = stop_words.ENGLISH_STOP_WORDS, \n",
    "                             ngram_range=(2, 4))\n",
    "text_countn = countn_vectorizer.fit_transform(reviews_df.text)\n",
    "text_countn_features = countn_vectorizer.get_feature_names()\n",
    "text_countn_df = pd.SparseDataFrame(text_countn)\n",
    "text_countn_df.columns = text_countn_features\n",
    "text_countn_df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_countn_totals = text_countn_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.barh(text_countn_totals[11::-1].keys(),text_countn_totals[11::-1], 0.5,  align='center')\n",
    "plt.title('Top Ten Bigram in Reviews')\n",
    "plt.ylabel('Phrases')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good_reviews = reviews_df[reviews_df.stars >= 4.0]\n",
    "good_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textg_countn = countn_vectorizer.fit_transform(good_reviews.text)\n",
    "textg_countn_features = countn_vectorizer.get_feature_names()\n",
    "textg_countn_df = pd.SparseDataFrame(textg_countn)\n",
    "textg_countn_df.columns = textg_countn_features\n",
    "textg_countn_df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textg_countn_totals = textg_countn_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.barh(textg_countn_totals[7::-1].keys(),textg_countn_totals[7::-1], 0.5,  align='center')\n",
    "plt.title('Top Bigrams in Good Reviews')\n",
    "plt.ylabel('Phrases')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bad_reviews = reviews_df[reviews_df.stars <= 2.0]\n",
    "bad_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textb_countn = countn_vectorizer.fit_transform(bad_reviews.text)\n",
    "textb_countn_features = countn_vectorizer.get_feature_names()\n",
    "textb_countn_df = pd.SparseDataFrame(textb_countn)\n",
    "textb_countn_df.columns = textb_countn_features\n",
    "textb_countn_df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textb_countn_totals = textb_countn_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.barh(textb_countn_totals[7::-1].keys(),textb_countn_totals[7::-1], 0.5,  align='center')\n",
    "plt.title('Top Bigrams in Bad Reviews')\n",
    "plt.ylabel('Phrases')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_sparse_df, \\\n",
    "                                                    reviews_df.stars, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "y_pred = pd.Series(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"MSE:\" + str(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R2:\"+ str(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ridgereg = linear_model.Ridge()\n",
    "alpha_range = [10, 1, 0.1, 0.01, 0.001]\n",
    "ridge_params = {'alpha': alpha_range}\n",
    "ridge_grid = GridSearchCV(ridgereg, ridge_params, cv=10, scoring='neg_mean_squared_error', n_jobs = 4, return_train_score=True)\n",
    "ridge_grid.fit(X_train, np.ravel(y_train))\n",
    "ridge_results = pd.DataFrame(ridge_grid.cv_results_)\n",
    "ridge_results.sort_values(by='rank_test_score').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regL = linear_model.LinearRegression()\n",
    "regL.fit(X_train, y_train)\n",
    "y_pred_regL = regL.predict(X_test)\n",
    "y_pred_regL = pd.Series(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"MSE:\" + str(mean_squared_error(y_test, y_pred_regL)))\n",
    "print(\"R2:\"+ str(r2_score(y_test, y_pred_regL)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print( \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_components_range = [10,15,20,25,30]\n",
    "learning_decay_range = [0.5,0.7,0.9]\n",
    "search_params = {'n_components': n_components_range, 'learning_decay': learning_decay_range}\n",
    "\n",
    "lda = LatentDirichletAllocation(max_iter=10, learning_method='online')\n",
    "lda_model = GridSearchCV(lda, param_grid=search_params, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "lda_model.fit(text_count_df)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "elapse = end-start\n",
    "print(elapse.seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model\n",
    "best_lda_model = lda_model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", lda_model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", lda_model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(text_count_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(text_count_df)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(text_count_df))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no_top_words = 2\n",
    "display_topics(best_lda_model, text_count_features, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control for price, location, type of restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pandas-dev/pandas/issues/18152\n",
    "\n",
    "max_records = 1e5\n",
    "df = pd.read_json('yelp_dataset/review.json', lines=True, chunksize=max_records)\n",
    "reviews = pd.DataFrame() # Initialize the dataframe\n",
    "try:\n",
    "   for df_chunk in df:\n",
    "       reviews = pd.concat([reviews, df_chunk])\n",
    "except ValueError:\n",
    "       print ('\\nSome messages in the file cannot be parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6685900, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "      <td>1</td>\n",
       "      <td>Q1sbwvVQXV2734tPgoKj4Q</td>\n",
       "      <td>1</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>6</td>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "      <td>0</td>\n",
       "      <td>GJXCdrto3ASJOqKeVWPi6Q</td>\n",
       "      <td>5</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yXQM5uF2jS6es16SJzNHfg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>0</td>\n",
       "      <td>2TzJjDVDEuAW6MR5Vuc1ug</td>\n",
       "      <td>5</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>3</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ikCg8xy5JIg_NGPx-MSIDA</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "      <td>0</td>\n",
       "      <td>yi0R0Ugj_xUx_Nek0-_Qig</td>\n",
       "      <td>5</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>0</td>\n",
       "      <td>dacAIZ6fTM6mqwW5uxkskg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "      <td>0</td>\n",
       "      <td>11a8sVPMUFtaC7_ABRkmtw</td>\n",
       "      <td>1</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>7</td>\n",
       "      <td>ssoyf2_x0EQMed6fgHeMyQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  ujmEBvifdJM6h6RLv4wQIg     0 2013-05-07 04:34:36      1   \n",
       "1  NZnhc2sEQy3RmzKTZnqtwQ     0 2017-01-14 21:30:33      0   \n",
       "2  WTqjgwHlXbSFevF32_DJVw     0 2016-11-09 20:09:03      0   \n",
       "3  ikCg8xy5JIg_NGPx-MSIDA     0 2018-01-09 20:56:38      0   \n",
       "4  b1b1eb3uo-w561D0ZfCEiQ     0 2018-01-30 23:07:38      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  Q1sbwvVQXV2734tPgoKj4Q      1   \n",
       "1  GJXCdrto3ASJOqKeVWPi6Q      5   \n",
       "2  2TzJjDVDEuAW6MR5Vuc1ug      5   \n",
       "3  yi0R0Ugj_xUx_Nek0-_Qig      5   \n",
       "4  11a8sVPMUFtaC7_ABRkmtw      1   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Total bill for this horrible service? Over $8G...       6   \n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ...       0   \n",
       "2  I have to say that this office really has it t...       3   \n",
       "3  Went in for a lunch. Steak sandwich was delici...       0   \n",
       "4  Today was my second out of three sessions I ha...       7   \n",
       "\n",
       "                  user_id  \n",
       "0  hG7b0MtEbXx5QzbzE6C_VA  \n",
       "1  yXQM5uF2jS6es16SJzNHfg  \n",
       "2  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "3  dacAIZ6fTM6mqwW5uxkskg  \n",
       "4  ssoyf2_x0EQMed6fgHeMyQ  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(reviews.shape)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = reviews.join(business, lsuffix='_r', rsuffix='_b', how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192609, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id_r</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars_r</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>address</th>\n",
       "      <th>...</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars_b</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "      <td>1</td>\n",
       "      <td>Q1sbwvVQXV2734tPgoKj4Q</td>\n",
       "      <td>1</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>6</td>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "      <td>2818 E Camino Acequia Drive</td>\n",
       "      <td>...</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>33.522143</td>\n",
       "      <td>-112.018481</td>\n",
       "      <td>Arizona Biltmore Golf Club</td>\n",
       "      <td>85016</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "      <td>0</td>\n",
       "      <td>GJXCdrto3ASJOqKeVWPi6Q</td>\n",
       "      <td>5</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yXQM5uF2jS6es16SJzNHfg</td>\n",
       "      <td>30 Eglinton Avenue W</td>\n",
       "      <td>...</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>{'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...</td>\n",
       "      <td>1</td>\n",
       "      <td>43.605499</td>\n",
       "      <td>-79.652289</td>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "      <td>L5R 3E7</td>\n",
       "      <td>128</td>\n",
       "      <td>2.5</td>\n",
       "      <td>ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>0</td>\n",
       "      <td>2TzJjDVDEuAW6MR5Vuc1ug</td>\n",
       "      <td>5</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>3</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>10110 Johnston Rd, Ste 15</td>\n",
       "      <td>...</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>{'Monday': '17:30-21:30', 'Wednesday': '17:30-...</td>\n",
       "      <td>1</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "      <td>28210</td>\n",
       "      <td>170</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ikCg8xy5JIg_NGPx-MSIDA</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "      <td>0</td>\n",
       "      <td>yi0R0Ugj_xUx_Nek0-_Qig</td>\n",
       "      <td>5</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>0</td>\n",
       "      <td>dacAIZ6fTM6mqwW5uxkskg</td>\n",
       "      <td>15655 W Roosevelt St, Ste 237</td>\n",
       "      <td>...</td>\n",
       "      <td>Goodyear</td>\n",
       "      <td>{'Monday': '8:0-17:0', 'Tuesday': '8:0-17:0', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.455613</td>\n",
       "      <td>-112.395596</td>\n",
       "      <td>Farmers Insurance - Paul Lorenz</td>\n",
       "      <td>85338</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "      <td>0</td>\n",
       "      <td>11a8sVPMUFtaC7_ABRkmtw</td>\n",
       "      <td>1</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>7</td>\n",
       "      <td>ssoyf2_x0EQMed6fgHeMyQ</td>\n",
       "      <td>4209 Stuart Andrew Blvd, Ste F</td>\n",
       "      <td>...</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>{'Monday': '7:0-23:0', 'Tuesday': '7:0-23:0', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>35.190012</td>\n",
       "      <td>-80.887223</td>\n",
       "      <td>Queen City Plumbing</td>\n",
       "      <td>28217</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            business_id_r  cool                date  funny  \\\n",
       "0  ujmEBvifdJM6h6RLv4wQIg     0 2013-05-07 04:34:36      1   \n",
       "1  NZnhc2sEQy3RmzKTZnqtwQ     0 2017-01-14 21:30:33      0   \n",
       "2  WTqjgwHlXbSFevF32_DJVw     0 2016-11-09 20:09:03      0   \n",
       "3  ikCg8xy5JIg_NGPx-MSIDA     0 2018-01-09 20:56:38      0   \n",
       "4  b1b1eb3uo-w561D0ZfCEiQ     0 2018-01-30 23:07:38      0   \n",
       "\n",
       "                review_id  stars_r  \\\n",
       "0  Q1sbwvVQXV2734tPgoKj4Q        1   \n",
       "1  GJXCdrto3ASJOqKeVWPi6Q        5   \n",
       "2  2TzJjDVDEuAW6MR5Vuc1ug        5   \n",
       "3  yi0R0Ugj_xUx_Nek0-_Qig        5   \n",
       "4  11a8sVPMUFtaC7_ABRkmtw        1   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Total bill for this horrible service? Over $8G...       6   \n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ...       0   \n",
       "2  I have to say that this office really has it t...       3   \n",
       "3  Went in for a lunch. Steak sandwich was delici...       0   \n",
       "4  Today was my second out of three sessions I ha...       7   \n",
       "\n",
       "                  user_id                         address  ...          city  \\\n",
       "0  hG7b0MtEbXx5QzbzE6C_VA     2818 E Camino Acequia Drive  ...       Phoenix   \n",
       "1  yXQM5uF2jS6es16SJzNHfg            30 Eglinton Avenue W  ...   Mississauga   \n",
       "2  n6-Gk65cPZL6Uz8qRm3NYw       10110 Johnston Rd, Ste 15  ...     Charlotte   \n",
       "3  dacAIZ6fTM6mqwW5uxkskg   15655 W Roosevelt St, Ste 237  ...      Goodyear   \n",
       "4  ssoyf2_x0EQMed6fgHeMyQ  4209 Stuart Andrew Blvd, Ste F  ...     Charlotte   \n",
       "\n",
       "                                               hours is_open   latitude  \\\n",
       "0                                               None       0  33.522143   \n",
       "1  {'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...       1  43.605499   \n",
       "2  {'Monday': '17:30-21:30', 'Wednesday': '17:30-...       1  35.092564   \n",
       "3  {'Monday': '8:0-17:0', 'Tuesday': '8:0-17:0', ...       1  33.455613   \n",
       "4  {'Monday': '7:0-23:0', 'Tuesday': '7:0-23:0', ...       1  35.190012   \n",
       "\n",
       "    longitude                             name  postal_code  review_count  \\\n",
       "0 -112.018481       Arizona Biltmore Golf Club        85016             5   \n",
       "1  -79.652289       Emerald Chinese Restaurant      L5R 3E7           128   \n",
       "2  -80.859132      Musashi Japanese Restaurant        28210           170   \n",
       "3 -112.395596  Farmers Insurance - Paul Lorenz        85338             3   \n",
       "4  -80.887223              Queen City Plumbing        28217             4   \n",
       "\n",
       "  stars_b state  \n",
       "0     3.0    AZ  \n",
       "1     2.5    ON  \n",
       "2     4.0    NC  \n",
       "3     5.0    AZ  \n",
       "4     4.0    NC  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(joined.shape)\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59371, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id_r</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars_r</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>address</th>\n",
       "      <th>...</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars_b</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "      <td>0</td>\n",
       "      <td>GJXCdrto3ASJOqKeVWPi6Q</td>\n",
       "      <td>5</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yXQM5uF2jS6es16SJzNHfg</td>\n",
       "      <td>30 Eglinton Avenue W</td>\n",
       "      <td>...</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>{'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...</td>\n",
       "      <td>1</td>\n",
       "      <td>43.605499</td>\n",
       "      <td>-79.652289</td>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "      <td>L5R 3E7</td>\n",
       "      <td>128</td>\n",
       "      <td>2.5</td>\n",
       "      <td>ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>0</td>\n",
       "      <td>2TzJjDVDEuAW6MR5Vuc1ug</td>\n",
       "      <td>5</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>3</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>10110 Johnston Rd, Ste 15</td>\n",
       "      <td>...</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>{'Monday': '17:30-21:30', 'Wednesday': '17:30-...</td>\n",
       "      <td>1</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "      <td>28210</td>\n",
       "      <td>170</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mRUVMJkUGxrByzMQ2MuOpA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-15 23:27:08</td>\n",
       "      <td>1</td>\n",
       "      <td>-I5umRTkhw15RqpKMl_o1Q</td>\n",
       "      <td>1</td>\n",
       "      <td>Walked in around 4 on a Friday afternoon, we s...</td>\n",
       "      <td>0</td>\n",
       "      <td>-mA3-1mN4JIEkqOtdbNXCQ</td>\n",
       "      <td>2450 E Indian School Rd</td>\n",
       "      <td>...</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>{'Monday': '7:0-0:0', 'Tuesday': '7:0-0:0', 'W...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.495194</td>\n",
       "      <td>-112.028588</td>\n",
       "      <td>Taco Bell</td>\n",
       "      <td>85016</td>\n",
       "      <td>18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LUN6swQYa4xJKaM_UEUOEw</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-27 20:25:26</td>\n",
       "      <td>0</td>\n",
       "      <td>qlXw1JQ0UodW7qrmVgwCXw</td>\n",
       "      <td>4</td>\n",
       "      <td>Michael from Red Carpet VIP is amazing ! I rea...</td>\n",
       "      <td>0</td>\n",
       "      <td>bAhqAPoWaZYcyYi7bs024Q</td>\n",
       "      <td>5981 Andrews Rd</td>\n",
       "      <td>...</td>\n",
       "      <td>Mentor-on-the-Lake</td>\n",
       "      <td>{'Monday': '10:0-0:0', 'Tuesday': '10:0-0:0', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>41.708520</td>\n",
       "      <td>-81.359556</td>\n",
       "      <td>Marco's Pizza</td>\n",
       "      <td>44060</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cHdJXLlKNWixBXpDwEGb_A</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-04-01 16:30:00</td>\n",
       "      <td>7</td>\n",
       "      <td>6BnQwlxRn7ZuWdzninM9sQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I love chinese food and I love mexican food. W...</td>\n",
       "      <td>1</td>\n",
       "      <td>JSrP-dUmLlwZiI7Dp3PQ2A</td>\n",
       "      <td>1775 E Tropicana Ave, Ste 29</td>\n",
       "      <td>...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>36.100016</td>\n",
       "      <td>-115.128529</td>\n",
       "      <td>Carluccio's Tivoli Gardens</td>\n",
       "      <td>89119</td>\n",
       "      <td>40</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             business_id_r  cool                date  funny  \\\n",
       "1   NZnhc2sEQy3RmzKTZnqtwQ     0 2017-01-14 21:30:33      0   \n",
       "2   WTqjgwHlXbSFevF32_DJVw     0 2016-11-09 20:09:03      0   \n",
       "11  mRUVMJkUGxrByzMQ2MuOpA     0 2017-12-15 23:27:08      1   \n",
       "13  LUN6swQYa4xJKaM_UEUOEw     0 2018-04-27 20:25:26      0   \n",
       "17  cHdJXLlKNWixBXpDwEGb_A     1 2015-04-01 16:30:00      7   \n",
       "\n",
       "                 review_id  stars_r  \\\n",
       "1   GJXCdrto3ASJOqKeVWPi6Q        5   \n",
       "2   2TzJjDVDEuAW6MR5Vuc1ug        5   \n",
       "11  -I5umRTkhw15RqpKMl_o1Q        1   \n",
       "13  qlXw1JQ0UodW7qrmVgwCXw        4   \n",
       "17  6BnQwlxRn7ZuWdzninM9sQ        3   \n",
       "\n",
       "                                                 text  useful  \\\n",
       "1   I *adore* Travis at the Hard Rock's new Kelly ...       0   \n",
       "2   I have to say that this office really has it t...       3   \n",
       "11  Walked in around 4 on a Friday afternoon, we s...       0   \n",
       "13  Michael from Red Carpet VIP is amazing ! I rea...       0   \n",
       "17  I love chinese food and I love mexican food. W...       1   \n",
       "\n",
       "                   user_id                       address  ...   \\\n",
       "1   yXQM5uF2jS6es16SJzNHfg          30 Eglinton Avenue W  ...    \n",
       "2   n6-Gk65cPZL6Uz8qRm3NYw     10110 Johnston Rd, Ste 15  ...    \n",
       "11  -mA3-1mN4JIEkqOtdbNXCQ       2450 E Indian School Rd  ...    \n",
       "13  bAhqAPoWaZYcyYi7bs024Q               5981 Andrews Rd  ...    \n",
       "17  JSrP-dUmLlwZiI7Dp3PQ2A  1775 E Tropicana Ave, Ste 29  ...    \n",
       "\n",
       "                  city                                              hours  \\\n",
       "1          Mississauga  {'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...   \n",
       "2            Charlotte  {'Monday': '17:30-21:30', 'Wednesday': '17:30-...   \n",
       "11             Phoenix  {'Monday': '7:0-0:0', 'Tuesday': '7:0-0:0', 'W...   \n",
       "13  Mentor-on-the-Lake  {'Monday': '10:0-0:0', 'Tuesday': '10:0-0:0', ...   \n",
       "17           Las Vegas                                               None   \n",
       "\n",
       "   is_open   latitude   longitude                         name  postal_code  \\\n",
       "1        1  43.605499  -79.652289   Emerald Chinese Restaurant      L5R 3E7   \n",
       "2        1  35.092564  -80.859132  Musashi Japanese Restaurant        28210   \n",
       "11       1  33.495194 -112.028588                    Taco Bell        85016   \n",
       "13       1  41.708520  -81.359556                Marco's Pizza        44060   \n",
       "17       0  36.100016 -115.128529   Carluccio's Tivoli Gardens        89119   \n",
       "\n",
       "    review_count stars_b state  \n",
       "1            128     2.5    ON  \n",
       "2            170     4.0    NC  \n",
       "11            18     3.0    AZ  \n",
       "13            16     4.0    OH  \n",
       "17            40     4.0    NV  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = joined['categories'].str.contains('Restaurants', regex=False)\n",
    "temp = temp.fillna(False)\n",
    "joined = joined[temp]\n",
    "print(joined.shape)\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joined_lv = joined[joined['city']==\"Las Vegas\"]\n",
    "joined_lv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lv_restaurants\n",
    "restaurants = joined_lv['categories'].str.contains('Restaurants', regex=False)\n",
    "restaurants = restaurants.fillna(False)\n",
    "restaurants = joined_lv[restaurants]\n",
    "print(restaurants.shape)\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restaurants.name.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categories_df = pd.Series(restaurants['categories']).str.get_dummies(sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categories_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "totals = categories_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.barh(totals[11:1:-1].keys(),totals[11:1:-1], 0.5,  align='center')\n",
    "plt.title('Top Ten Restaurants Categories')\n",
    "plt.ylabel('Categories')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only look at nightlife in Las Vegas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lv_restaurants\n",
    "nightlife = joined_lv['categories'].str.contains('Nightlife', regex=False)\n",
    "nightlife = nightlife.fillna(False)\n",
    "nightlife = joined_lv[nightlife]\n",
    "print(nightlife.shape)\n",
    "nightlife.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "nightlife.text = nightlife.text.apply(text_process)\n",
    "nightlife.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keep the size to 1000 features for size\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "nightlife.text = nightlife.text.apply(text_process)\n",
    "vectorizer = TfidfVectorizer(analyzer='word',min_df=10, stop_words = stop_words.ENGLISH_STOP_WORDS, \n",
    "                             ngram_range=(1, 2))\n",
    "text_tfidf = vectorizer.fit_transform(nightlife.text)\n",
    "text_features = vectorizer.get_feature_names()\n",
    "text_sparse_df = pd.SparseDataFrame(text_tfidf)\n",
    "text_sparse_df.columns = text_features\n",
    "text_sparse_df.fillna(0.0, inplace=True)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "elapse = end-start\n",
    "\n",
    "print(elapse.seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "nightlife.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://www.geeksforgeeks.org/part-speech-tagging-stop-words-using-nltk-python/\n",
    "CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: â€œthere isâ€ â€¦ think of it like â€œthere existsâ€)\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective â€˜bigâ€™\n",
    "JJR adjective, comparative â€˜biggerâ€™\n",
    "JJS adjective, superlative â€˜biggestâ€™\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular â€˜deskâ€™\n",
    "NNS noun plural â€˜desksâ€™\n",
    "NNP proper noun, singular â€˜Harrisonâ€™\n",
    "NNPS proper noun, plural â€˜Americansâ€™\n",
    "PDT predeterminer â€˜all the kidsâ€™\n",
    "POS possessive ending parentâ€˜s\n",
    "PRP personal pronoun I, he, she\n",
    "PRP$ possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO to go â€˜toâ€˜ the store.\n",
    "UH interjection errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle taken\n",
    "VBP verb, sing. present, non-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WP$ possessive wh-pronoun whose\n",
    "WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.w3schools.com/python/ref_string_join.asp\n",
    "def tokenize(string):\n",
    "    d = []\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    for x in tags:\n",
    "        if x[1] == \"NN\" or x[1] == \"NNS\" or x[1] == \"NNP\" or x[1] == \"NNPS\":\n",
    "            d.append(x[0])\n",
    "    d = \" \".join(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize(\"This is a sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "nightlife['text'] = nightlife['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nightlife[\"text\"].str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word',min_df=5, stop_words = stop_words.ENGLISH_STOP_WORDS, \n",
    "                             ngram_range=(1, 1))\n",
    "text_count = count_vectorizer.fit_transform(nightlife.text)\n",
    "text_count_features = count_vectorizer.get_feature_names()\n",
    "text_count_df = pd.SparseDataFrame(text_count)\n",
    "text_count_df.columns = text_count_features\n",
    "text_count_df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_count_totals = text_count_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "plt.barh(text_count_totals[11::-1].keys(),text_count_totals[11::-1], 0.5,  align='center')\n",
    "plt.title('Top Ten Words in Reviews')\n",
    "plt.ylabel('Words')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "countn_vectorizer = CountVectorizer(analyzer='word',min_df=5, stop_words = stop_words.ENGLISH_STOP_WORDS, \n",
    "                             ngram_range=(2, 4))\n",
    "text_countn = countn_vectorizer.fit_transform(nightlife.text)\n",
    "text_countn_features = countn_vectorizer.get_feature_names()\n",
    "text_countn_df = pd.SparseDataFrame(text_countn)\n",
    "text_countn_df.columns = text_countn_features\n",
    "text_countn_df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_countn_totals = text_countn_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.barh(text_countn_totals[11::-1].keys(),text_countn_totals[11::-1], 0.5,  align='center')\n",
    "plt.title('Top Ten Bigram in Reviews')\n",
    "plt.ylabel('Phrases')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good_reviews = nightlife[nightlife.stars_r >= 4.0]\n",
    "good_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textg_countn = countn_vectorizer.fit_transform(good_reviews.text)\n",
    "textg_countn_features = countn_vectorizer.get_feature_names()\n",
    "textg_countn_df = pd.SparseDataFrame(textg_countn)\n",
    "textg_countn_df.columns = textg_countn_features\n",
    "textg_countn_df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textg_countn_totals = textg_countn_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.barh(textg_countn_totals[7::-1].keys(),textg_countn_totals[7::-1], 0.5,  align='center')\n",
    "plt.title('Top Bigrams in Good Reviews')\n",
    "plt.ylabel('Phrases')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bad_reviews = nightlife[nightlife.stars_r <= 2.0]\n",
    "bad_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textb_countn = countn_vectorizer.fit_transform(bad_reviews.text)\n",
    "textb_countn_features = countn_vectorizer.get_feature_names()\n",
    "textb_countn_df = pd.SparseDataFrame(textb_countn)\n",
    "textb_countn_df.columns = textb_countn_features\n",
    "textb_countn_df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textb_countn_totals = textb_countn_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.barh(textb_countn_totals[7::-1].keys(),textb_countn_totals[7::-1], 0.5,  align='center')\n",
    "plt.title('Top Bigrams in Bad Reviews')\n",
    "plt.ylabel('Phrases')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_and_loc(cat, loc):\n",
    "    #filter location\n",
    "    joined_loc = joined[joined['city']==loc]\n",
    "    \n",
    "    #filter restaurants\n",
    "    restaurants = joined_loc['categories'].str.contains('Restaurants', regex=False)\n",
    "    restaurants = restaurants.fillna(False)\n",
    "    restaurants = joined_loc[restaurants]\n",
    "    \n",
    "    #filter category\n",
    "    category = joined_loc['categories'].str.contains(cat, regex=False)\n",
    "    category = category.fillna(False)\n",
    "    category = joined_loc[category]\n",
    "    \n",
    "    #text manipulation\n",
    "    category.text = category.text.apply(text_process)\n",
    "\n",
    "    ## keep the size to 1000 features for size\n",
    "\n",
    "    category.text = category.text.apply(text_process)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word',min_df=10, stop_words = stop_words.ENGLISH_STOP_WORDS, \n",
    "                                 ngram_range=(1, 2))\n",
    "    text_tfidf = vectorizer.fit_transform(category.text)\n",
    "    text_features = vectorizer.get_feature_names()\n",
    "    text_sparse_df = pd.SparseDataFrame(text_tfidf)\n",
    "    text_sparse_df.columns = text_features\n",
    "    text_sparse_df.fillna(0.0, inplace=True)\n",
    "    \n",
    "    category.text = category.text.apply(tokenize)\n",
    "    \n",
    "    count_vectorizer = CountVectorizer(analyzer='word',min_df=5, stop_words = stop_words.ENGLISH_STOP_WORDS, \n",
    "                             ngram_range=(1, 1))\n",
    "    text_count = count_vectorizer.fit_transform(category.text)\n",
    "    text_count_features = count_vectorizer.get_feature_names()\n",
    "    text_count_df = pd.SparseDataFrame(text_count)\n",
    "    text_count_df.columns = text_count_features\n",
    "    text_count_df.fillna(0.0, inplace=True)\n",
    "\n",
    "    text_count_totals = text_count_df.sum().sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(25,20))\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.barh(text_count_totals[11::-1].keys(),text_count_totals[11::-1], 0.5,  align='center')\n",
    "    plt.title('Top Ten Words in Reviews')\n",
    "    plt.ylabel('Words')\n",
    "    plt.xlabel('Frequency')\n",
    "    \n",
    "    countn_vectorizer = CountVectorizer(analyzer='word',min_df=5, stop_words = stop_words.ENGLISH_STOP_WORDS, \n",
    "                             ngram_range=(2, 4))\n",
    "    text_countn = countn_vectorizer.fit_transform(category.text)\n",
    "    text_countn_features = countn_vectorizer.get_feature_names()\n",
    "    text_countn_df = pd.SparseDataFrame(text_countn)\n",
    "    text_countn_df.columns = text_countn_features\n",
    "    text_countn_df.fillna(0.0, inplace=True)\n",
    "    \n",
    "    text_countn_totals = text_countn_df.sum().sort_values(ascending=False)\n",
    "    \n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.barh(text_countn_totals[11::-1].keys(),text_countn_totals[11::-1], 0.5,  align='center')\n",
    "    plt.title('Top Ten Bigram in Reviews')\n",
    "    plt.ylabel('Phrases')\n",
    "    plt.xlabel('Frequency')\n",
    "    \n",
    "    good_reviews = category[category.stars_r >= 4.0]\n",
    "    textg_countn = countn_vectorizer.fit_transform(good_reviews.text)\n",
    "    textg_countn_features = countn_vectorizer.get_feature_names()\n",
    "    textg_countn_df = pd.SparseDataFrame(textg_countn)\n",
    "    textg_countn_df.columns = textg_countn_features\n",
    "    textg_countn_df.fillna(0.0, inplace=True)\n",
    "    textg_countn_totals = textg_countn_df.sum().sort_values(ascending=False)\n",
    "    \n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.barh(textg_countn_totals[7::-1].keys(),textg_countn_totals[7::-1], 0.5,  align='center')\n",
    "    plt.title('Top Bigrams in Good Reviews')\n",
    "    plt.ylabel('Phrases')\n",
    "    plt.xlabel('Frequency')\n",
    "    \n",
    "    bad_reviews = category[category.stars_r <= 2.0]\n",
    "    textb_countn = countn_vectorizer.fit_transform(bad_reviews.text)\n",
    "    textb_countn_features = countn_vectorizer.get_feature_names()\n",
    "    textb_countn_df = pd.SparseDataFrame(textb_countn)\n",
    "    textb_countn_df.columns = textb_countn_features\n",
    "    textb_countn_df.fillna(0.0, inplace=True)\n",
    "    textb_countn_totals = textb_countn_df.sum().sort_values(ascending=False)\n",
    "    \n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.barh(textb_countn_totals[7::-1].keys(),textb_countn_totals[7::-1], 0.5,  align='center')\n",
    "    plt.title('Top Bigrams in Bad Reviews')\n",
    "    plt.ylabel('Phrases')\n",
    "    plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_and_loc(\"Nightlife\", \"Las Vegas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_and_loc(\"Mexican\", \"Las Vegas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_and_loc(\"Breakfast & Brunch\", \"Las Vegas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_and_loc(\"Mexican\", \"Phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_and_loc(\"Sushi\", \"Toronto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_and_loc(\"Sushi\", \"Las Vegas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_and_loc(\"Pizza\", \"Toronto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_and_loc(\"Pizza\", \"Las Vegas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = joined[['text', 'city']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp, joined['stars_r'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<47496x982 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2530959 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = X_train['text']\n",
    "vectorizer = TfidfVectorizer(min_df = 0.01)\n",
    "\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "tf_train = vectorizer.transform(corpus)\n",
    "tf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11875x982 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 636571 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test data\n",
    "test_corpus = X_test['text']\n",
    "tf_test = vectorizer.transform(test_corpus)\n",
    "tf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf_train = pd.SparseDataFrame(tf_train.toarray(),columns=vectorizer.get_feature_names(),index=X_trainC.index)\n",
    "tf_test = pd.SparseDataFrame(tf_test.toarray(),columns=vectorizer.get_feature_names(),index=X_testC.index)\n",
    "tf_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#X_train['city'] = X_train['city'].fillna(\" \")\n",
    "#X_test['city'] = X_test['city'].fillna(\" \")\n",
    "\n",
    "X_train['city'] = X_train['city'].str.lower()\n",
    "X_test['city'] = X_test['city'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None,\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "       n_values=None, sparse=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(X_train.shape)\n",
    "#X_trainC = pd.get_dummies(X_train['city'], dummy_na=True).to_sparse()\n",
    "#X_testC = pd.get_dummies(X_test['city'], dummy_na=True).to_sparse()\n",
    "#print(X_trainC.shape)\n",
    "#X_testC.head()\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(X_train['city'].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<47496x684 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 47496 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainC = enc.transform(X_train['city'].reshape(-1,1))\n",
    "X_testC = enc.transform(X_test['city'].reshape(-1,1))\n",
    "X_trainC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainJ = hstack([tf_train, X_trainC], format=\"csr\")\n",
    "X_testJ = hstack([tf_test, X_testC], format=\"csr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = tf_train.join(X_trainC, lsuffix='_tf', rsuffix='_c', how=\"inner\")\n",
    "#print(X_train.shape)\n",
    "#X_test = tf_test.join(X_testC, lsuffix='_tf', rsuffix='_c', how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"alpha\":[0.0001, 0.001, 0.01, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge_grid = GridSearchCV(estimator=ridge, param_grid=parameters, cv=10, scoring=\"r2\", n_jobs=20)\n",
    "print(ridge_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "ridge_grid.fit(X_trainJ, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(ridge_grid.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = ridge_grid.cv_results_['mean_test_score']\n",
    "stds = ridge_grid.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, ridge_grid.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_grid = GridSearchCV(estimator=lasso, param_grid=parameters, cv=10, scoring=\"r2\", n_jobs=20)\n",
    "print(lasso_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_grid.fit(X_trainJ, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(lasso_grid.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = lasso_grid.cv_results_['mean_test_score']\n",
    "stds = lasso_grid.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, lasso_grid.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet = ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_grid = GridSearchCV(estimator=ElasticNet, param_grid=parameters, cv=10, scoring=\"r2\", n_jobs=20)\n",
    "print(en_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_grid.fit(X_trainJ, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(en_grid.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = en_grid.cv_results_['mean_test_score']\n",
    "stds = en_grid.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, en_grid.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.0001, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_final = ElasticNet(alpha=0.0001)\n",
    "en_final.fit(X_trainJ, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.1585687 , 3.72691187, 3.94407391, ..., 0.08736341, 3.70186236,\n",
       "       3.57353303])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = en_final.predict(X_testJ)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6210968619978035"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "\n",
    "    text_tf = vectorizer.transform(text)\n",
    "\n",
    "    #text_lower = text.str.lower()\n",
    "    text_lower = text[0].lower()\n",
    "\n",
    "    #text_enc = enc.transform(text_lower.reshape(-1,1))\n",
    "    text_enc = enc.transform([[text_lower]])\n",
    "\n",
    "    text_joined = hstack([text_tf, text_enc], format=\"csr\")\n",
    "    return en_final.predict(text_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.29413308]\n",
      "[4.30127942]\n",
      "[2.34088606]\n"
     ]
    }
   ],
   "source": [
    "print(predict([\"this is a review\"]))\n",
    "print(predict([\"This is a great restaurant!\"]))\n",
    "print(predict([\"This is a bad restaurant!\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pickle.dumps(en_final)\n",
    "p = pickle.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_predict(text):\n",
    "\n",
    "    text_tf = vectorizer.transform(text)\n",
    "\n",
    "    #text_lower = text.str.lower()\n",
    "    text_lower = text[0].lower()\n",
    "\n",
    "    #text_enc = enc.transform(text_lower.reshape(-1,1))\n",
    "    text_enc = enc.transform([[text_lower]])\n",
    "\n",
    "    text_joined = hstack([text_tf, text_enc], format=\"csr\")\n",
    "    return p.predict(text_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.29413308]\n",
      "[4.30127942]\n",
      "[2.34088606]\n"
     ]
    }
   ],
   "source": [
    "print(pickle_predict([\"this is a review\"]))\n",
    "print(pickle_predict([\"This is a great restaurant!\"]))\n",
    "print(pickle_predict([\"This is a bad restaurant!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_predict(text):\n",
    "\n",
    "    text_tf = vectorizer.transform(text)\n",
    "\n",
    "    #text_lower = text.str.lower()\n",
    "    text_lower = text[0].lower()\n",
    "\n",
    "    #text_enc = enc.transform(text_lower.reshape(-1,1))\n",
    "    text_enc = enc.transform([[text_lower]])\n",
    "\n",
    "    text_joined = hstack([text_tf, text_enc], format=\"csr\")\n",
    "    return en_final.predict(text_joined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
